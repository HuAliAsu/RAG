"""
Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
"""

import re
import hazm
from typing import Dict, List
from pathlib import Path


class TextPreprocessor:
    """
    Ú©Ù„Ø§Ø³ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
    """

    def __init__(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡"""
        self.normalizer = hazm.Normalizer()
        self.word_tokenizer = hazm.WordTokenizer()
        self.sent_tokenizer = hazm.SentenceTokenizer()

    def clean_text(self, text: str, remove_old_tags: bool = True,
                   remove_metadata: bool = True) -> str:
        """
        Ù†Ø±Ù…Ø§Ù„ÛŒØ²Ù‡ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ù…ØªÙ†

        Args:
            text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
            remove_old_tags: Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ (@@-0000-@@)
            remove_metadata: Ø­Ø°Ù metadata Ù‚Ø¨Ù„ÛŒ

        Returns:
            str: Ù…ØªÙ† Ù¾Ø§Ú© Ø´Ø¯Ù‡
        """
        if not text:
            return ""

        # Ù…Ø±Ø­Ù„Ù‡ 1: Ù†Ø±Ù…Ø§Ù„ÛŒØ²Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        text = self.normalizer.normalize(text)

        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
        if remove_old_tags:
            # ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯: @@-0000-@@
            text = re.sub(r'@@-\d{4}-@@', '', text)

            # ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø¯ÛŒÚ¯Ø±
            text = re.sub(r'@@.*?@@', '', text)

            # Ø®Ø·ÙˆØ· Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡
            text = re.sub(r'^[â”€=\-*_]{20,}$', '', text, flags=re.MULTILINE)

        # Ù…Ø±Ø­Ù„Ù‡ 3: Ø­Ø°Ù metadata Ù‚Ø¨Ù„ÛŒ
        if remove_metadata:
            # metadata Ø¯Ø± Ø¨Ø±Ø§Ú©Øª Ù…Ø±Ø¨Ø¹: [Ú©Ù„Ù…Ø§Øª: 450 | Ø¬Ù…Ù„Ø§Øª: 12]
            text = re.sub(r'\[.*?\]', '', text)

            # metadata Ø¨Ø§ pipe: Ú©Ù„Ù…Ø§Øª: 450 | Ø¬Ù…Ù„Ø§Øª: 12
            text = re.sub(r'(Ú©Ù„Ù…Ø§Øª|Ø¬Ù…Ù„Ø§Øª|Stage|Ø§Ø·Ù…ÛŒÙ†Ø§Ù†|Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡):.*?\|', '', text)

        # Ù…Ø±Ø­Ù„Ù‡ 4: Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ¶Ø§Ù‡Ø§
        # ÙØ¶Ø§Ù‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ
        text = re.sub(r' {2,}', ' ', text)

        # ØªØ¨â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ
        text = re.sub(r'\t+', ' ', text)

        # Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ø¨ÛŒØ´ Ø§Ø² 2 Ø®Ø·
        text = re.sub(r'\n{3,}', '\n\n', text)

        # ÙØ¶Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§/Ø§Ù†ØªÙ‡Ø§ÛŒ Ø®Ø·ÙˆØ·
        lines = text.split('\n')
        lines = [line.strip() for line in lines]
        text = '\n'.join(lines)

        # Ù…Ø±Ø­Ù„Ù‡ 5: Trim Ù†Ù‡Ø§ÛŒÛŒ
        text = text.strip()

        return text

    def extract_doc_info(self, text: str) -> Dict:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù…Ø§Ø±ÛŒ Ø§Ø² Ù…ØªÙ†

        Args:
            text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ

        Returns:
            dict: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        """
        if not text:
            return {
                'word_count': 0,
                'char_count': 0,
                'sentence_count': 0,
                'paragraph_count': 0,
                'avg_words_per_sentence': 0.0,
                'avg_words_per_paragraph': 0.0
            }

        # Ø´Ù…Ø§Ø±Ø´ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§
        char_count = len(text)

        # Ø´Ù…Ø§Ø±Ø´ Ú©Ù„Ù…Ø§Øª
        words = self.word_tokenizer.tokenize(text)
        word_count = len(words)

        # Ø´Ù…Ø§Ø±Ø´ Ø¬Ù…Ù„Ø§Øª
        try:
            sentences = self.sent_tokenizer.tokenize(text)
            sentence_count = len(sentences)
        except:
            # fallback Ø³Ø§Ø¯Ù‡
            sentence_count = text.count('.') + text.count('!') + text.count('?')

        # Ø´Ù…Ø§Ø±Ø´ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§
        paragraphs = [p for p in text.split('\n') if p.strip()]
        paragraph_count = len(paragraphs)

        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§
        avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0.0
        avg_words_per_paragraph = word_count / paragraph_count if paragraph_count > 0 else 0.0

        return {
            'word_count': word_count,
            'char_count': char_count,
            'sentence_count': sentence_count,
            'paragraph_count': paragraph_count,
            'avg_words_per_sentence': round(avg_words_per_sentence, 1),
            'avg_words_per_paragraph': round(avg_words_per_paragraph, 1)
        }

    def split_into_paragraphs(self, text: str) -> List[str]:
        """
        ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§

        Args:
            text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ

        Returns:
            list: Ù„ÛŒØ³Øª Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ)
        """
        paragraphs = text.split('\n')
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        return paragraphs

    def clean_pattern_text(self, text: str) -> str:
        """
        Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ† Ø§Ù„Ú¯ÙˆÙ‡Ø§ (Ø§Ø² TextArea)

        Args:
            text: Ù…ØªÙ† Ø§Ù„Ú¯ÙˆÙ‡Ø§

        Returns:
            str: Ù…ØªÙ† Ù¾Ø§Ú© Ø´Ø¯Ù‡
        """
        # Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()
            # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ùˆ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§
            if line and not line.startswith('#'):
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)


# ØªØ³Øª
if __name__ == "__main__":
    print("ğŸ§ª ØªØ³Øª preprocessor.py\n")

    preprocessor = TextPreprocessor()

    # Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡
    sample_text = """
    @@-0001-@@

    Ø§ÛŒÙ†    ÛŒÚ©   Ù…ØªÙ†     ØªØ³Øª Ø§Ø³Øª.


    Ø¨Ø§ ÙØ¶Ø§Ù‡Ø§ÛŒ    Ø§Ø¶Ø§ÙÛŒ Ùˆ Ø®Ø·ÙˆØ·    Ø®Ø§Ù„ÛŒ.

    [Ú©Ù„Ù…Ø§Øª: 450 | Ø¬Ù…Ù„Ø§Øª: 12 | Stage: 1]

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø¯ÙˆÙ… Ø§Ø³Øª.
    """

    print("1ï¸âƒ£ Ù…ØªÙ† Ø§ØµÙ„ÛŒ:")
    print(repr(sample_text)[:100] + "...\n")

    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
    cleaned = preprocessor.clean_text(sample_text)
    print("2ï¸âƒ£ Ù…ØªÙ† Ù¾Ø§Ú© Ø´Ø¯Ù‡:")
    print(repr(cleaned) + "\n")

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
    info = preprocessor.extract_doc_info(cleaned)
    print("3ï¸âƒ£ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ØªÙ†:")
    for key, value in info.items():
        print(f"   {key}: {value}")