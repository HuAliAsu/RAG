"""
Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ú†Ø§Ù†Ú©ÛŒÙ†Ú¯ Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ø¯Ùˆ Ù„Ø§ÛŒÙ‡
Parent-Child Chunking Strategy
"""

import re
import numpy as np
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import hazm


class OptimizedHybridChunker:
    """
    Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ú†Ø§Ù†Ú©ÛŒÙ†Ú¯ Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Parent-Child

    Stage 1: Pattern-Based Detection (Ø³Ø±ÛŒØ¹)
    Stage 2: Coherence Analysis (Ù…ØªÙˆØ³Ø·)
    Stage 3: Semantic Embedding (Ø¯Ù‚ÛŒÙ‚)
    """

    def __init__(self, config: Dict):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡

        Args:
            config: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø´Ø§Ù…Ù„:
                - structural_patterns: Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ
                - semantic_patterns: Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
                - special_keywords: Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
                - min_chunk_size: Ø­Ø¯Ø§Ù‚Ù„ Ú©Ù„Ù…Ø§Øª
                - max_chunk_size: Ø­Ø¯Ø§Ú©Ø«Ø± Ú©Ù„Ù…Ø§Øª
                - overlap_size: Ú©Ù„Ù…Ø§Øª overlap
                - coherence_threshold: Ø¢Ø³ØªØ§Ù†Ù‡ coherence
                - similarity_threshold: Ø¢Ø³ØªØ§Ù†Ù‡ similarity
                - enable_stage1/2/3: ÙØ¹Ø§Ù„/ØºÛŒØ±ÙØ¹Ø§Ù„ Stageâ€ŒÙ‡Ø§
                - enable_parent_child: ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø¯Ùˆ Ù„Ø§ÛŒÙ‡
        """
        self.config = config

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ù†Ø¯Ø§Ø²Ù‡
        self.min_chunk_size = config.get('min_chunk_size', 150)
        self.max_chunk_size = config.get('max_chunk_size', 800)
        self.overlap_size = config.get('overlap_size', 50)

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Stageâ€ŒÙ‡Ø§
        self.enable_stage1 = config.get('enable_stage1', True)
        self.enable_stage2 = config.get('enable_stage2', True)
        self.enable_stage3 = config.get('enable_stage3', True)

        # Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§
        self.coherence_threshold = config.get('coherence_threshold', 0.15)
        self.similarity_threshold = config.get('similarity_threshold', 0.75)

        # Ø³ÛŒØ³ØªÙ… Ø¯Ùˆ Ù„Ø§ÛŒÙ‡
        self.enable_parent_child = config.get('enable_parent_child', True)
        self.child_chunk_size = config.get('child_chunk_size', 100)

        # Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†
        self.normalizer = hazm.Normalizer()
        self.word_tokenizer = hazm.WordTokenizer()
        self.sent_tokenizer = hazm.SentenceTokenizer()

        # TF-IDF vectorizer Ø¨Ø±Ø§ÛŒ Stage 2
        self.vectorizer = TfidfVectorizer(max_features=100)

        # Embedder (Ø§Ø² ÙØ§Ø² 1)
        from src.core.embedder import OllamaEmbedder
        self.embedder = OllamaEmbedder(
            model=config.get('embedding_model', 'embeddinggemma:latest')
        )

        # Ø§Ù„Ú¯ÙˆÙ‡Ø§
        self.patterns = self._parse_patterns(config)

        # Ø¢Ù…Ø§Ø±
        self.stats = {
            'stage1_boundaries': 0,
            'stage2_boundaries': 0,
            'stage3_boundaries': 0,
            'embedding_calls': 0,
            'start_time': 0,
            'end_time': 0
        }

    def _parse_patterns(self, config: Dict) -> Dict[str, List[Dict]]:
        """Ù¾Ø§Ø±Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø§Ø² config"""
        from src.utils.validators import validate_pattern_line
        from src.utils.logger import debug, warning

        patterns = {
            'structural': [],
            'semantic': [],
            'special': []
        }

        # Ù¾Ø§Ø±Ø³ Ù‡Ø± Ù†ÙˆØ¹ Ø§Ù„Ú¯Ùˆ
        for pattern_type, config_key in [
            ('structural', 'structural_patterns'),
            ('semantic', 'semantic_patterns'),
            ('special', 'special_keywords')
        ]:
            pattern_text = config.get(config_key, '')

            if isinstance(pattern_text, str):
                lines = pattern_text.strip().split('\n')
            elif isinstance(pattern_text, list):
                lines = pattern_text
            else:
                continue

            for line in lines:
                is_valid, error, parsed = validate_pattern_line(line)

                if is_valid and parsed:
                    # Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ regex
                    try:
                        parsed['regex_compiled'] = re.compile(parsed['regex'])
                        patterns[pattern_type].append(parsed)
                        debug(f"   Ø§Ù„Ú¯ÙˆÛŒ {pattern_type}: {parsed['name']}")
                    except re.error as e:
                        warning(f"   Ø®Ø·Ø§ Ø¯Ø± Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ regex '{parsed['name']}': {e}")

        return patterns

    def chunk_text(self, text: str, headings: Optional[List[Dict]] = None,
                   source_metadata: Optional[Dict] = None) -> Tuple[List[Dict], Dict]:
        """
        Ù…ØªØ¯ Ø§ØµÙ„ÛŒ Ú†Ø§Ù†Ú©ÛŒÙ†Ú¯

        Args:
            text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
            headings: Ù„ÛŒØ³Øª Ø¹Ù†Ø§ÙˆÛŒÙ† (Ø§Ø² Word)
            source_metadata: metadata ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ

        Returns:
            tuple: (chunks, processing_stats)
        """
        from src.utils.logger import info, debug
        from src.core.preprocessor import TextPreprocessor

        self.stats['start_time'] = time.time()

        info("\n" + "="*70)
        info("ğŸ”„ Ø´Ø±ÙˆØ¹ Chunking Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ")
        info("="*70)

        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        preprocessor = TextPreprocessor()
        text = preprocessor.clean_text(text, remove_old_tags=True)

        paragraphs = text.split('\n')
        paragraphs = [p.strip() for p in paragraphs if p.strip()]

        info(f"ğŸ“ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§: {len(paragraphs)}")

        # Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ¬Ù…ÛŒØ¹ Ù…Ø±Ø²Ù‡Ø§
        all_boundaries = []

        # Ø§ÙØ²ÙˆØ¯Ù† Headingâ€ŒÙ‡Ø§
        if headings:
            info(f"\nğŸ“‘ {len(headings)} Ø¹Ù†ÙˆØ§Ù† Heading Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")
            for heading in headings:
                all_boundaries.append({
                    'index': heading['para_index'],
                    'score': 5.0,
                    'confidence': 1.0,
                    'text': heading['text'],
                    'signals': ['heading'],
                    'stage': 0,
                    'stage_name': 'Heading',
                    'heading_level': heading['level'],
                    'priority': 1
                })

        # Stage 1: Pattern Detection
        if self.enable_stage1:
            stage1_boundaries = self._stage1_pattern_detection(paragraphs)
            all_boundaries.extend(stage1_boundaries['confirmed'])
            uncertain_boundaries = stage1_boundaries['uncertain']
        else:
            info("\nâ­ï¸ Stage 1 ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª")
            uncertain_boundaries = []

        # Stage 2: Coherence Analysis
        if self.enable_stage2 and uncertain_boundaries:
            stage2_boundaries = self._stage2_coherence_analysis(paragraphs, uncertain_boundaries)
            all_boundaries.extend(stage2_boundaries['confirmed'])
            uncertain_boundaries = stage2_boundaries['uncertain']
        elif not self.enable_stage2:
            info("\nâ­ï¸ Stage 2 ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª")

        # Stage 3: Semantic Embedding
        if self.enable_stage3 and uncertain_boundaries and self.embedder.available:
            stage3_boundaries = self._stage3_semantic_verification(paragraphs, uncertain_boundaries)
            all_boundaries.extend(stage3_boundaries)
        elif not self.enable_stage3:
            info("\nâ­ï¸ Stage 3 ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª")
        elif not self.embedder.available:
            info("\nâš ï¸ Stage 3: Ollama Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª")

        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        all_boundaries.sort(key=lambda x: x['index'])

        # Ø³Ø§Ø®Øª chunkâ€ŒÙ‡Ø§
        info("\nâœ‚ï¸ Ø³Ø§Ø®Øª chunkâ€ŒÙ‡Ø§...")
        chunks = self._create_chunks_from_boundaries(
            paragraphs, all_boundaries, source_metadata
        )

        # Ø³ÛŒØ³ØªÙ… Parent-Child
        if self.enable_parent_child:
            info("\nğŸ‘¨â€ğŸ‘¦ Ø³Ø§Ø®Øª Ø³Ø§Ø®ØªØ§Ø± Parent-Child...")
            chunks = self._create_parent_child_structure(chunks)

        # Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        self.stats['end_time'] = time.time()
        self.stats['total_time'] = self.stats['end_time'] - self.stats['start_time']
        self.stats['total_chunks'] = len(chunks)

        self._print_stats()

        processing_stats = self.stats.copy()

        return chunks, processing_stats

