"""
Ø³ÛŒØ³ØªÙ… Chunking Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
Optimized Hybrid: Pattern + Coherence + Semantic Embedding
Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡: Ú©Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ RAG
"""

import re
import numpy as np
from typing import List, Dict, Tuple, Optional
from docx import Document
from docx.enum.style import WD_STYLE_TYPE
import hazm
from pathlib import Path
import requests
import json
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class OllamaEmbedding:
    """Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ollama"""

    def __init__(self, model: str = "embeddinggemma", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url
        self.embed_url = f"{base_url}/api/embeddings"

    def get_embedding(self, text: str) -> Optional[np.ndarray]:
        """Ø¯Ø±ÛŒØ§ÙØª embedding Ø§Ø² Ollama"""
        try:
            response = requests.post(
                self.embed_url,
                json={"model": self.model, "prompt": text},
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return np.array(result['embedding'])
            else:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª embedding: {response.status_code}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama: {e}")
            return None

    def get_batch_embeddings(self, texts: List[str], show_progress: bool = True) -> List[Optional[np.ndarray]]:
        """Ø¯Ø±ÛŒØ§ÙØª embedding Ø¨Ø±Ø§ÛŒ Ù„ÛŒØ³Øª Ù…ØªÙ†â€ŒÙ‡Ø§"""
        embeddings = []
        total = len(texts)

        for idx, text in enumerate(texts):
            if show_progress and (idx % 10 == 0 or idx == total - 1):
                print(f"  Embedding: {idx + 1}/{total}", end='\r')

            emb = self.get_embedding(text)
            embeddings.append(emb)

        if show_progress:
            print()  # Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ø¨Ø¹Ø¯ Ø§Ø² progress

        return embeddings


class OptimizedHybridChunker:
    """Chunker Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡"""

    def __init__(
            self,
            min_chunk_size: int = 200,
            max_chunk_size: int = 800,
            overlap_size: int = 50,
            ollama_model: str = "embeddinggemma"
    ):
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size

        # Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†
        self.normalizer = hazm.Normalizer()
        self.sent_tokenizer = hazm.SentenceTokenizer()
        self.word_tokenizer = hazm.WordTokenizer()

        # Ollama embedding
        self.embedder = OllamaEmbedding(model=ollama_model)

        # Ø¢Ù…Ø§Ø±
        self.stats = {
            'stage1_boundaries': 0,
            'stage2_boundaries': 0,
            'stage3_boundaries': 0,
            'embedding_calls': 0
        }

    def normalize_text(self, text: str) -> str:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
        text = self.normalizer.normalize(text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()

    # ============================================
    # STAGE 1: Fast Pattern-Based Detection
    # ============================================

    def stage1_pattern_detection(self, paragraphs: List[str]) -> List[Dict]:
        """Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ´Ø®ÛŒØµ Ø³Ø±ÛŒØ¹ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§"""
        boundaries = []

        for idx, para in enumerate(paragraphs):
            if not para.strip():
                continue

            score = 0.0
            confidence = 0.0
            signals = []

            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù‚ÙˆÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ (confidence: 1.0)
            structural_patterns = [
                (r'^(ÙØµÙ„|Ø¨Ø®Ø´)\s+[\dÛ°-Û¹]+', 1.0, 'chapter'),
                (r'^Ø¯Ø§Ø³ØªØ§Ù†\s+[\dÛ°-Û¹]+', 1.0, 'story_title'),
                (r'^[\dÛ°-Û¹]+[\.\)]\s+', 0.9, 'numbered'),
                (r'^={3,}|^-{3,}|^\*{3,}', 0.95, 'separator'),
                (r'^(Ø§ÙˆÙ„|Ø¯ÙˆÙ…|Ø³ÙˆÙ…|Ú†Ù‡Ø§Ø±Ù…|Ù¾Ù†Ø¬Ù…|Ø´Ø´Ù…|Ù‡ÙØªÙ…|Ù‡Ø´ØªÙ…|Ù†Ù‡Ù…|Ø¯Ù‡Ù…)\s*[-:]\s*', 0.9, 'ordinal'),
            ]

            for pattern, conf, signal in structural_patterns:
                if re.search(pattern, para):
                    score += 3.0
                    confidence = max(confidence, conf)
                    signals.append(signal)
                    break

            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù…ØªÙˆØ³Ø· (confidence: 0.6-0.8)
            semantic_patterns = [
                # Ø´Ø±ÙˆØ¹ Ø¯Ø§Ø³ØªØ§Ù†
                (r'(Ø±ÙˆØ²ÛŒ\s+Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ|ÛŒÚ©ÛŒ\s+Ø¨ÙˆØ¯\s+ÛŒÚ©ÛŒ\s+Ù†Ø¨ÙˆØ¯)', 0.7, 'story_start'),
                (r'Ø¯Ø±\s+(Ø²Ù…Ø§Ù†ÛŒ|Ø±ÙˆØ²ÛŒ|Ø´Ù‡Ø±ÛŒ|Ø¯ÛŒØ§Ø±ÛŒ)\s+Ú©Ù‡', 0.65, 'context_start'),

                # Ù¾Ø§ÛŒØ§Ù† Ø¨Ø®Ø´
                (r'(Ø¯Ø±Ø³|Ù†Ú©ØªÙ‡|Ø¢Ù…ÙˆØ®ØªÙ‡|Ø¹Ø¨Ø±Øª)[:\s]+', 0.75, 'lesson'),
                (r'Ø¨Ù‡\s+Ø§ÛŒÙ†\s+ØªØ±ØªÛŒØ¨', 0.6, 'conclusion'),

                # Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯ÙØªÙ…Ø§Ù†
                (r'^(Ø­Ø§Ù„|Ø§Ú©Ù†ÙˆÙ†|Ø¨Ú¯Ø°Ø§Ø±ÛŒØ¯)\s+', 0.65, 'discourse_marker'),
                (r'^(Ø§Ù…Ø§|ÙˆÙ„ÛŒ|Ø¨Ø§\s+Ø§ÛŒÙ†\s+Ø­Ø§Ù„)\s+', 0.6, 'contrast'),
            ]

            for pattern, conf, signal in semantic_patterns:
                if re.search(pattern, para):
                    score += 2.0
                    confidence = max(confidence, conf)
                    signals.append(signal)

            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù (confidence: 0.5-0.7)
            words = self.word_tokenizer.tokenize(para)
            para_length = len(words)

            # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ú©ÙˆØªØ§Ù‡ + Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ù†Ø§Ø³Ø¨
            if para_length < 15 and idx > 0:
                prev_para = paragraphs[idx - 1] if idx > 0 else ""
                next_para = paragraphs[idx + 1] if idx < len(paragraphs) - 1 else ""

                if len(prev_para.strip()) > 50 and len(next_para.strip()) > 50:
                    score += 1.5
                    confidence = max(confidence, 0.7)
                    signals.append('short_title')

            # Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯
            has_empty_before = idx > 0 and not paragraphs[idx - 1].strip()
            has_empty_after = idx < len(paragraphs) - 1 and not paragraphs[idx + 1].strip()

            if has_empty_before and has_empty_after and para_length < 20:
                score += 1.5
                confidence = max(confidence, 0.65)
                signals.append('isolated')

            # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø®ÛŒÙ„ÛŒ Ø¨Ù„Ù†Ø¯ (Ø§Ø­ØªÙ…Ø§Ù„ ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹)
            if para_length > 200:
                score += 0.5
                confidence = max(confidence, 0.4)
                signals.append('long_para')

            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø±Ø²
            if score > 0:
                boundaries.append({
                    'index': idx,
                    'score': score,
                    'confidence': confidence,
                    'text': para,
                    'signals': signals,
                    'stage': 1
                })

        # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ confidence
        high_conf = [b for b in boundaries if b['confidence'] >= 0.8]
        medium_conf = [b for b in boundaries if 0.5 <= b['confidence'] < 0.8]
        low_conf = [b for b in boundaries if b['confidence'] < 0.5]

        self.stats['stage1_boundaries'] = len(high_conf)

        print(f"  Stage 1: {len(high_conf)} Ù…Ø±Ø² Ù‚Ø·Ø¹ÛŒ | {len(medium_conf)} Ù…Ø±Ø² Ø§Ø­ØªÙ…Ø§Ù„ÛŒ | {len(low_conf)} Ù…Ø±Ø² Ø¶Ø¹ÛŒÙ")

        return {
            'confirmed': high_conf,
            'uncertain': medium_conf + low_conf,
            'all': boundaries
        }

    # ============================================
    # STAGE 2: Coherence Analysis
    # ============================================

    def stage2_coherence_analysis(self, paragraphs: List[str], uncertain_boundaries: List[Dict]) -> List[Dict]:
        """Ù…Ø±Ø­Ù„Ù‡ 2: ØªØ­Ù„ÛŒÙ„ Ø§Ù†Ø³Ø¬Ø§Ù… Ù…ØªÙ†"""

        if not uncertain_boundaries:
            return []

        confirmed = []
        still_uncertain = []

        for boundary in uncertain_boundaries:
            idx = boundary['index']

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ coherence Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² Ù…Ø±Ø²
            before_coherence = self._calculate_coherence(paragraphs, max(0, idx - 3), idx)
            after_coherence = self._calculate_coherence(paragraphs, idx, min(len(paragraphs), idx + 3))
            cross_coherence = self._calculate_coherence(paragraphs, max(0, idx - 2), min(len(paragraphs), idx + 2))

            # Ø§Ú¯Ø± coherence Ø¯Ø±ÙˆÙ† Ø¨Ø®Ø´â€ŒÙ‡Ø§ Ø¨Ø§Ù„Ø§ Ùˆ Ø¨ÛŒÙ† Ø¨Ø®Ø´â€ŒÙ‡Ø§ Ù¾Ø§ÛŒÛŒÙ† â†’ Ù…Ø±Ø² Ù‚Ø·Ø¹ÛŒ
            coherence_drop = (before_coherence + after_coherence) / 2 - cross_coherence

            if coherence_drop > 0.15:  # Ø¢Ø³ØªØ§Ù†Ù‡ ØªØ¬Ø±Ø¨ÛŒ
                boundary['confidence'] = 0.8
                boundary['coherence_drop'] = coherence_drop
                boundary['stage'] = 2
                confirmed.append(boundary)
            elif coherence_drop > 0.08:
                boundary['coherence_drop'] = coherence_drop
                still_uncertain.append(boundary)
            # else: Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯

        self.stats['stage2_boundaries'] = len(confirmed)

        print(f"  Stage 2: {len(confirmed)} Ù…Ø±Ø² ØªØ£ÛŒÛŒØ¯ Ø´Ø¯ | {len(still_uncertain)} Ù…Ø±Ø² Ù†ÛŒØ§Ø² Ø¨Ù‡ embedding")

        return {
            'confirmed': confirmed,
            'uncertain': still_uncertain
        }

    def _calculate_coherence(self, paragraphs: List[str], start: int, end: int) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø³Ø¬Ø§Ù… Ù…ØªÙ†ÛŒ Ø¨ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§"""

        if start >= end or start < 0 or end > len(paragraphs):
            return 0.0

        section = ' '.join([p for p in paragraphs[start:end] if p.strip()])

        if not section:
            return 0.0

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        words = self.word_tokenizer.tokenize(section)

        # Ø­Ø°Ù stop words
        stop_words = {'Ùˆ', 'Ø¯Ø±', 'Ø¨Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø±Ø§', 'Ø¨Ø§', 'Ø¨Ø±Ø§ÛŒ', 'Ø¢Ù†', 'ÛŒÚ©', 'Ø§Ø³Øª', 'Ø´ÙˆØ¯', 'Ù…ÛŒ'}
        filtered_words = [w for w in words if len(w) > 2 and w not in stop_words]

        if len(filtered_words) < 5:
            return 0.0

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÚ©Ø±Ø§Ø± Ú©Ù„Ù…Ø§Øª (Ù†Ø´Ø§Ù†Ù‡ Ø§Ù†Ø³Ø¬Ø§Ù…)
        word_freq = Counter(filtered_words)
        repeated_words = sum(1 for count in word_freq.values() if count > 1)

        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        coherence = repeated_words / len(set(filtered_words)) if filtered_words else 0

        return min(coherence, 1.0)

    # ============================================
    # STAGE 3: Semantic Embedding
    # ============================================

    def stage3_semantic_verification(self, paragraphs: List[str], uncertain_boundaries: List[Dict]) -> List[Dict]:
        """Ù…Ø±Ø­Ù„Ù‡ 3: ØªØ£ÛŒÛŒØ¯ Ø¨Ø§ embedding Ù…Ø¹Ù†Ø§ÛŒÛŒ"""

        if not uncertain_boundaries:
            print("  Stage 3: Ù‡ÛŒÚ† Ù…Ø±Ø² Ù†Ø§Ù…Ø´Ø®ØµÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ³Øª")
            return []

        print(f"  Stage 3: Ø¨Ø±Ø±Ø³ÛŒ {len(uncertain_boundaries)} Ù…Ø±Ø² Ø¨Ø§ embedding...")

        confirmed = []

        # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø±Ø² Ù†Ø§Ù…Ø´Ø®Øµ
        for boundary in uncertain_boundaries:
            idx = boundary['index']

            # Ù…ØªÙ† Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² Ù…Ø±Ø²
            before_text = ' '.join([p for p in paragraphs[max(0, idx - 2):idx] if p.strip()])
            after_text = ' '.join([p for p in paragraphs[idx:min(len(paragraphs), idx + 3)] if p.strip()])

            if not before_text or not after_text:
                continue

            # Ø¯Ø±ÛŒØ§ÙØª embedding
            emb_before = self.embedder.get_embedding(before_text)
            emb_after = self.embedder.get_embedding(after_text)

            self.stats['embedding_calls'] += 2

            if emb_before is None or emb_after is None:
                print(f"    âš ï¸ Ø®Ø·Ø§ Ø¯Ø± embedding Ø¨Ø±Ø§ÛŒ Ù…Ø±Ø² {idx}")
                continue

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
            similarity = np.dot(emb_before, emb_after) / (np.linalg.norm(emb_before) * np.linalg.norm(emb_after))

            # Ø§Ú¯Ø± Ø´Ø¨Ø§Ù‡Øª Ù¾Ø§ÛŒÛŒÙ† â†’ Ù…Ø±Ø² Ù‚Ø·Ø¹ÛŒ
            if similarity < 0.75:  # Ø¢Ø³ØªØ§Ù†Ù‡ ØªØ¬Ø±Ø¨ÛŒ
                boundary['confidence'] = 0.85
                boundary['semantic_similarity'] = float(similarity)
                boundary['stage'] = 3
                confirmed.append(boundary)

        self.stats['stage3_boundaries'] = len(confirmed)

        print(f"  Stage 3: {len(confirmed)} Ù…Ø±Ø² ØªØ£ÛŒÛŒØ¯ Ø´Ø¯ Ø¨Ø§ embedding")

        return confirmed

    # ============================================
    # Main Processing Pipeline
    # ============================================

    def create_chunks(self, text: str, headings: List[Dict] = None) -> List[Dict]:
        """Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ† Ø§ØµÙ„ÛŒ chunking Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ"""

        print("\nğŸ”„ Ø´Ø±ÙˆØ¹ Chunking Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ...")

        text = self.normalize_text(text)
        paragraphs = text.split('\n')

        # Ø§ÙØ²ÙˆØ¯Ù† headingâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø±Ø²Ù‡Ø§ÛŒ Ù‚Ø·Ø¹ÛŒ
        confirmed_boundaries = []

        if headings:
            print(f"  ğŸ“‘ {len(headings)} Ø¹Ù†ÙˆØ§Ù† (Heading) ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
            for heading in headings:
                confirmed_boundaries.append({
                    'index': heading['para_index'],
                    'score': 5.0,
                    'confidence': 1.0,
                    'text': heading['text'],
                    'signals': ['heading'],
                    'stage': 0,  # Ø§Ø² ÙØ§ÛŒÙ„ ÙˆØ±Ø¯
                    'heading_level': heading['level']
                })

        # Stage 1: Pattern Detection
        print("\nâš¡ Stage 1: Pattern Detection (Ø³Ø±ÛŒØ¹)")
        stage1_result = self.stage1_pattern_detection(paragraphs)
        confirmed_boundaries.extend(stage1_result['confirmed'])

        # Stage 2: Coherence Analysis
        print("\nğŸ” Stage 2: Coherence Analysis (Ù…ØªÙˆØ³Ø·)")
        stage2_result = self.stage2_coherence_analysis(paragraphs, stage1_result['uncertain'])
        confirmed_boundaries.extend(stage2_result['confirmed'])

        # Stage 3: Semantic Embedding
        print("\nğŸ§  Stage 3: Semantic Embedding (Ø¯Ù‚ÛŒÙ‚)")
        stage3_result = self.stage3_semantic_verification(paragraphs, stage2_result['uncertain'])
        confirmed_boundaries.extend(stage3_result)

        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ index
        confirmed_boundaries.sort(key=lambda x: x['index'])

        # Ø§ÛŒØ¬Ø§Ø¯ chunkâ€ŒÙ‡Ø§
        print("\nâœ‚ï¸ Ø§ÛŒØ¬Ø§Ø¯ chunkâ€ŒÙ‡Ø§...")
        chunks = self._create_chunks_from_boundaries(paragraphs, confirmed_boundaries)

        # Ø§ÙØ²ÙˆØ¯Ù† overlap
        chunks = self._add_smart_overlap(chunks)

        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        chunks = self._optimize_chunks(chunks)

        # Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        self._print_stats(len(chunks))

        return chunks

    def _create_chunks_from_boundaries(self, paragraphs: List[str], boundaries: List[Dict]) -> List[Dict]:
        """Ø§ÛŒØ¬Ø§Ø¯ chunkâ€ŒÙ‡Ø§ Ø§Ø² Ù…Ø±Ø²Ù‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡"""

        chunks = []
        boundary_indices = [b['index'] for b in boundaries]
        boundary_indices = [0] + boundary_indices + [len(paragraphs)]

        for i in range(len(boundary_indices) - 1):
            start_idx = boundary_indices[i]
            end_idx = boundary_indices[i + 1]

            section = '\n'.join([p for p in paragraphs[start_idx:end_idx] if p.strip()])

            if not section.strip():
                continue

            # metadata Ù…Ø±Ø²
            boundary_info = None
            if i < len(boundaries):
                boundary_info = boundaries[i]

            # Ø§ÛŒØ¬Ø§Ø¯ chunk
            chunk = self._create_chunk_dict(section, len(chunks), boundary_info)
            chunks.append(chunk)

        return chunks

    def _create_chunk_dict(self, text: str, chunk_id: int, boundary_info: Optional[Dict]) -> Dict:
        """Ø§ÛŒØ¬Ø§Ø¯ dictionary Ø¨Ø±Ø§ÛŒ chunk"""

        words = self.word_tokenizer.tokenize(text)
        sentences = self.sent_tokenizer.tokenize(text)

        chunk = {
            'chunk_id': chunk_id,
            'text': text,
            'word_count': len(words),
            'sentence_count': len(sentences),
            'keywords': self._extract_keywords(text),
            'overlap_prev': '',
            'overlap_next': ''
        }

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ø²
        if boundary_info:
            chunk['boundary_confidence'] = boundary_info.get('confidence', 0)
            chunk['boundary_stage'] = boundary_info.get('stage', 0)
            chunk['boundary_signals'] = boundary_info.get('signals', [])

            if 'heading_level' in boundary_info:
                chunk['heading_level'] = boundary_info['heading_level']

        return chunk

    def _add_smart_overlap(self, chunks: List[Dict]) -> List[Dict]:
        """Ø§ÙØ²ÙˆØ¯Ù† overlap Ù‡ÙˆØ´Ù…Ù†Ø¯"""

        for i in range(len(chunks) - 1):
            curr_chunk = chunks[i]
            next_chunk = chunks[i + 1]

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…Ù„Ø§Øª Ø¢Ø®Ø±
            curr_sents = self.sent_tokenizer.tokenize(curr_chunk['text'])

            # ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù…Ù„Ø§Øª overlap Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…Ø±Ø²
            confidence = next_chunk.get('boundary_confidence', 0.5)

            if confidence >= 0.9:  # Ù…Ø±Ø² Ù‚Ø·Ø¹ÛŒ â†’ overlap Ú©Ù…
                overlap_count = 1
            elif confidence >= 0.7:  # Ù…Ø±Ø² Ù…ØªÙˆØ³Ø· â†’ overlap Ù…ØªÙˆØ³Ø·
                overlap_count = 2
            else:  # Ù…Ø±Ø² Ø¶Ø¹ÛŒÙ â†’ overlap Ø²ÛŒØ§Ø¯
                overlap_count = 3

            overlap_sents = curr_sents[-overlap_count:] if len(curr_sents) >= overlap_count else curr_sents
            overlap_text = ' '.join(overlap_sents)

            curr_chunk['overlap_next'] = overlap_text
            next_chunk['overlap_prev'] = overlap_text

        return chunks

    def _optimize_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ chunkâ€ŒÙ‡Ø§"""

        optimized = []
        i = 0

        while i < len(chunks):
            chunk = chunks[i]

            # Ø§Ø¯ØºØ§Ù… chunkâ€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú©
            if chunk['word_count'] < self.min_chunk_size and i < len(chunks) - 1:
                next_chunk = chunks[i + 1]

                # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø§Ø¯ØºØ§Ù… Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³Øª
                if next_chunk['word_count'] < self.max_chunk_size:
                    merged_text = chunk['text'] + '\n\n' + next_chunk['text']
                    chunk['text'] = merged_text
                    chunk['word_count'] = len(self.word_tokenizer.tokenize(merged_text))
                    chunk['merged'] = True
                    optimized.append(chunk)
                    i += 2
                    continue

            # ØªÙ‚Ø³ÛŒÙ… chunkâ€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯
            if chunk['word_count'] > self.max_chunk_size:
                sub_chunks = self._split_large_chunk(chunk['text'], chunk['chunk_id'])
                optimized.extend(sub_chunks)
            else:
                optimized.append(chunk)

            i += 1

        # Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ IDâ€ŒÙ‡Ø§
        for idx, chunk in enumerate(optimized):
            chunk['chunk_id'] = idx
            chunk['total_chunks'] = len(optimized)

        return optimized

    def _split_large_chunk(self, text: str, start_id: int) -> List[Dict]:
        """ØªÙ‚Ø³ÛŒÙ… chunk Ø¨Ø²Ø±Ú¯"""

        sentences = self.sent_tokenizer.tokenize(text)
        sub_chunks = []
        current_chunk = []
        current_words = 0

        for sent in sentences:
            words = self.word_tokenizer.tokenize(sent)

            if current_words + len(words) > self.max_chunk_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                sub_chunks.append(self._create_chunk_dict(chunk_text, start_id + len(sub_chunks), None))

                # overlap
                current_chunk = current_chunk[-2:] if len(current_chunk) >= 2 else []
                current_words = sum(len(self.word_tokenizer.tokenize(s)) for s in current_chunk)

            current_chunk.append(sent)
            current_words += len(words)

        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            sub_chunks.append(self._create_chunk_dict(chunk_text, start_id + len(sub_chunks), None))

        return sub_chunks

    def _extract_keywords(self, text: str, top_n: int = 5) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ"""

        words = self.word_tokenizer.tokenize(text)
        stop_words = {'Ùˆ', 'Ø¯Ø±', 'Ø¨Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø±Ø§', 'Ø¨Ø§', 'Ø¨Ø±Ø§ÛŒ', 'Ø¢Ù†', 'ÛŒÚ©', 'Ø§Ø³Øª', 'Ø´ÙˆØ¯', 'Ù…ÛŒ', 'Ø®ÙˆØ¯'}

        word_freq = {}
        for word in words:
            if len(word) > 2 and word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1

        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:top_n]]

    def _print_stats(self, total_chunks: int):
        """Ú†Ø§Ù¾ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ"""

        print("\n" + "=" * 50)
        print("ğŸ“Š Ø¢Ù…Ø§Ø± Chunking")
        print("=" * 50)
        print(f"  Chunkâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ: {total_chunks}")
        print(f"  Ù…Ø±Ø²Ù‡Ø§ÛŒ Stage 1 (Pattern): {self.stats['stage1_boundaries']}")
        print(f"  Ù…Ø±Ø²Ù‡Ø§ÛŒ Stage 2 (Coherence): {self.stats['stage2_boundaries']}")
        print(f"  Ù…Ø±Ø²Ù‡Ø§ÛŒ Stage 3 (Embedding): {self.stats['stage3_boundaries']}")
        print(f"  ØªØ¹Ø¯Ø§Ø¯ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Embedding: {self.stats['embedding_calls']}")

        total_boundaries = sum([
            self.stats['stage1_boundaries'],
            self.stats['stage2_boundaries'],
            self.stats['stage3_boundaries']
        ])

        if total_boundaries > 0:
            stage3_percent = (self.stats['stage3_boundaries'] / total_boundaries) * 100
            print(f"  Ø¯Ø±ØµØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Embedding: {stage3_percent:.1f}%")

        print("=" * 50 + "\n")


def extract_headings_from_docx(doc_path: str) -> Tuple[str, List[Dict]]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ùˆ headingâ€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ Word"""

    doc = Document(doc_path)
    full_text_parts = []
    headings = []
    para_index = 0

    for para in doc.paragraphs:
        text = para.text.strip()

        if not text:
            full_text_parts.append('')
            para_index += 1
            continue

        # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø¨Ú© Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
        style_name = para.style.name.lower()

        # ØªØ´Ø®ÛŒØµ heading
        if 'heading' in style_name:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø·Ø­ heading
            level = 1
            if 'heading 1' in style_name:
                level = 1
            elif 'heading 2' in style_name:
                level = 2
            elif 'heading 3' in style_name:
                level = 3

            headings.append({
                'text': text,
                'level': level,
                'para_index': para_index
            })

        full_text_parts.append(text)
        para_index += 1

    full_text = '\n'.join(full_text_parts)

    return full_text, headings


def process_word_file(input_path: str, output_path: str = None, ollama_model: str = "embeddinggemma"):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Word Ø¨Ø§ Optimized Hybrid Chunker"""

    print(f"ğŸ“– Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {input_path}")

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ùˆ headingâ€ŒÙ‡Ø§
    full_text, headings = extract_headings_from_docx(input_path)

    if not full_text.strip():
        print("âŒ ÙØ§ÛŒÙ„ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª!")
        return None, None

    print(f"âœ“ Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {len(full_text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
    print(f"âœ“ {len(headings)} Ø¹Ù†ÙˆØ§Ù† (Heading) ÛŒØ§ÙØª Ø´Ø¯")

    # Chunking
    chunker = OptimizedHybridChunker(
        min_chunk_size=200,
        max_chunk_size=800,
        overlap_size=50,
        ollama_model=ollama_model
    )

    chunks = chunker.create_chunks(full_text, headings=headings)

    if not chunks:
        print("âŒ Ù‡ÛŒÚ† chunk Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø´Ø¯!")
        return None, None

    # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ
    if output_path is None:
        input_file = Path(input_path)
        output_path = input_file.parent / f"{input_file.stem}_chunked{input_file.suffix}"

    output_doc = Document()

    # Ù†ÙˆØ´ØªÙ† chunkâ€ŒÙ‡Ø§
    for chunk in chunks:
        # ØªÚ¯ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡
        tag = f"@@-{chunk['chunk_id']:04d}-@@"
        tag_para = output_doc.add_paragraph(tag)
        tag_para.runs[0].bold = True
        tag_para.runs[0].font.size = 140000  # 14pt

        # Ù…ØªÙ† chunk
        output_doc.add_paragraph(chunk['text'])

        # metadata
        meta_parts = [
            f"Ú©Ù„Ù…Ø§Øª: {chunk['word_count']}",
            f"Ø¬Ù…Ù„Ø§Øª: {chunk['sentence_count']}",
            f"Stage: {chunk.get('boundary_stage', 'N/A')}",
            f"Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {chunk.get('boundary_confidence', 0):.2f}"
        ]

        if chunk.get('keywords'):
            meta_parts.append(f"Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡: {', '.join(chunk['keywords'])}")

        if chunk.get('heading_level'):
            meta_parts.append(f"Heading Ø³Ø·Ø­ {chunk['heading_level']}")

        meta_text = " | ".join(meta_parts)
        meta_para = output_doc.add_paragraph(meta_text)
        meta_para.runs[0].italic = True
        meta_para.runs[0].font.size = 100000  # 10pt

        # overlap (Ø§Ø®ØªÛŒØ§Ø±ÛŒ - Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯)
        if chunk.get('overlap_next'):
            overlap_para = output_doc.add_paragraph(f"[Overlap: {chunk['overlap_next'][:100]}...]")
            overlap_para.runs[0].font.color.rgb = (150, 150, 150)
            overlap_para.runs[0].font.size = 90000  # 9pt

        # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† chunkâ€ŒÙ‡Ø§
        output_doc.add_paragraph()
        output_doc.add_paragraph("â”€" * 80)
        output_doc.add_paragraph()

    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„
    output_doc.save(output_path)

    print(f"\nâœ… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_path}")

    # Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
    total_words = sum(c['word_count'] for c in chunks)
    avg_words = total_words / len(chunks) if chunks else 0
    min_words = min(c['word_count'] for c in chunks) if chunks else 0
    max_words = max(c['word_count'] for c in chunks) if chunks else 0

    print("\nğŸ“ˆ Ø¢Ù…Ø§Ø± chunkâ€ŒÙ‡Ø§:")
    print(f"  ØªØ¹Ø¯Ø§Ø¯: {len(chunks)}")
    print(f"  Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„Ù…Ø§Øª: {avg_words:.0f}")
    print(f"  Ú©Ù…ØªØ±ÛŒÙ†: {min_words} Ú©Ù„Ù…Ø§Øª")
    print(f"  Ø¨ÛŒØ´ØªØ±ÛŒÙ†: {max_words} Ú©Ù„Ù…Ø§Øª")

    # ØªÙˆØ²ÛŒØ¹ Ø¨Ø± Ø§Ø³Ø§Ø³ stage
    stage_dist = {}
    for chunk in chunks:
        stage = chunk.get('boundary_stage', 'unknown')
        stage_dist[stage] = stage_dist.get(stage, 0) + 1

    print("\nğŸ¯ ØªÙˆØ²ÛŒØ¹ chunkâ€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙˆØ´ ØªØ´Ø®ÛŒØµ:")
    stage_names = {0: 'Heading', 1: 'Pattern', 2: 'Coherence', 3: 'Embedding'}
    for stage, count in sorted(stage_dist.items()):
        stage_name = stage_names.get(stage, 'Unknown')
        print(f"  {stage_name}: {count} chunk")

    return chunks, str(output_path)


def test_ollama_connection(model: str = "embeddinggemma"):
    """ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama"""

    print("ğŸ” ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama...")

    embedder = OllamaEmbedding(model=model)
    test_text = "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† ØªØ³Øª Ø§Ø³Øª"

    result = embedder.get_embedding(test_text)

    if result is not None:
        print(f"âœ… Ø§ØªØµØ§Ù„ Ù…ÙˆÙÙ‚! (embedding dimension: {len(result)})")
        return True
    else:
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama!")
        print("\nÙ„Ø·ÙØ§Ù‹ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:")
        print("1. Ollama Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ Ø§Ø³ØªØŸ")
        print("   $ ollama serve")
        print(f"2. Ù…Ø¯Ù„ {model} Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø§Ø³ØªØŸ")
        print(f"   $ ollama pull {model}")
        print("3. Ollama Ø±ÙˆÛŒ Ù¾ÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªØŸ (11434)")
        return False


# ===========================================
# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
# ===========================================

if __name__ == "__main__":
    import sys

    # ØªØ³Øª Ø§ØªØµØ§Ù„ Ø§ÙˆÙ„ÛŒÙ‡
    if not test_ollama_connection("embeddinggemma"):
        print("\nâš ï¸ Ø¨Ø¯ÙˆÙ† OllamaØŒ ÙÙ‚Ø· Stage 1 Ùˆ 2 Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø¯Ù‚Øª Ú©Ù…ØªØ±)")
        response = input("Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯ØŸ (y/n): ")
        if response.lower() != 'y':
            sys.exit(1)

    # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ
    input_file = "your_book.docx"  # ÙØ§ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯

    # ÛŒØ§ Ø§Ø² command line
    if len(sys.argv) > 1:
        input_file = sys.argv[1]

    try:
        chunks, output_file = process_word_file(
            input_file,
            ollama_model="embeddinggemma"
        )

        if chunks and output_file:
            print("\n" + "=" * 50)
            print("ğŸ‰ Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯!")
            print("=" * 50)

            # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡
            print("\nğŸ“ Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„ÛŒÙ† chunk:")
            print("-" * 50)
            first_chunk = chunks[0]
            print(f"ID: {first_chunk['chunk_id']}")
            print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª: {first_chunk['word_count']}")
            print(f"Ø±ÙˆØ´ ØªØ´Ø®ÛŒØµ: Stage {first_chunk.get('boundary_stage', 'N/A')}")
            print(f"Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {first_chunk.get('boundary_confidence', 0):.2%}")
            print(f"\nÙ…ØªÙ†:\n{first_chunk['text'][:300]}...")

            if first_chunk.get('keywords'):
                print(f"\nÚ©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ: {', '.join(first_chunk['keywords'])}")

            print("\n" + "=" * 50)
            print(f"ğŸ“‚ ÙØ§ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ: {output_file}")
            print("=" * 50)

    except FileNotFoundError:
        print(f"\nâŒ Ø®Ø·Ø§: ÙØ§ÛŒÙ„ '{input_file}' ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        print("\nØ±Ø§Ù‡Ù†Ù…Ø§:")
        print(f"  python {sys.argv[0]} path/to/your/file.docx")

    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§: {str(e)}")
        import traceback

        traceback.print_exc()


# ===========================================
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø§Ø¶Ø§ÙÛŒ
# ===========================================

def analyze_chunks(chunks: List[Dict]):
    """ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ chunkâ€ŒÙ‡Ø§"""

    if not chunks:
        print("Ù‡ÛŒÚ† chunkâ€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!")
        return

    print("\n" + "=" * 60)
    print("ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Chunkâ€ŒÙ‡Ø§")
    print("=" * 60)

    # Ø¢Ù…Ø§Ø± Ø§ÙˆÙ„ÛŒÙ‡
    total = len(chunks)
    total_words = sum(c['word_count'] for c in chunks)

    print(f"\n1ï¸âƒ£ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ:")
    print(f"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ chunkâ€ŒÙ‡Ø§: {total}")
    print(f"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú©Ù„Ù…Ø§Øª: {total_words:,}")
    print(f"   Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø¯Ø± chunk: {total_words / total:.1f}")

    # ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„
    print(f"\n2ï¸âƒ£ ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ chunkâ€ŒÙ‡Ø§:")

    ranges = [
        (0, 150, "Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú©"),
        (150, 300, "Ú©ÙˆÚ†Ú©"),
        (300, 400, "Ù…ØªÙˆØ³Ø·"),
        (400, 600, "Ø¨Ø²Ø±Ú¯"),
        (600, float('inf'), "Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯")
    ]

    for min_w, max_w, label in ranges:
        count = sum(1 for c in chunks if min_w <= c['word_count'] < max_w)
        if count > 0:
            percentage = (count / total) * 100
            print(
                f"   {label:15} ({min_w:3}-{max_w if max_w != float('inf') else 'âˆ':>3}): {count:3} chunk ({percentage:5.1f}%)")

    # ØªÙˆØ²ÛŒØ¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙˆØ´ ØªØ´Ø®ÛŒØµ
    print(f"\n3ï¸âƒ£ Ø±ÙˆØ´ ØªØ´Ø®ÛŒØµ Ù…Ø±Ø²Ù‡Ø§:")

    stage_names = {
        0: "ğŸ“‘ Heading (Ø§Ø² ÙˆØ±Ø¯)",
        1: "âš¡ Pattern (Ø§Ù„Ú¯Ùˆ)",
        2: "ğŸ” Coherence (Ø§Ù†Ø³Ø¬Ø§Ù…)",
        3: "ğŸ§  Embedding (Ù…Ø¹Ù†Ø§ÛŒÛŒ)"
    }

    stage_counts = {}
    for chunk in chunks:
        stage = chunk.get('boundary_stage', 'unknown')
        stage_counts[stage] = stage_counts.get(stage, 0) + 1

    for stage in sorted(stage_counts.keys()):
        count = stage_counts[stage]
        percentage = (count / total) * 100
        name = stage_names.get(stage, "â“ Ù†Ø§Ù…Ø´Ø®Øµ")
        print(f"   {name:30}: {count:3} ({percentage:5.1f}%)")

    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…Ø±Ø²Ù‡Ø§
    print(f"\n4ï¸âƒ£ ØªÙˆØ²ÛŒØ¹ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…Ø±Ø²Ù‡Ø§:")

    confidences = [c.get('boundary_confidence', 0) for c in chunks]
    high_conf = sum(1 for c in confidences if c >= 0.8)
    med_conf = sum(1 for c in confidences if 0.5 <= c < 0.8)
    low_conf = sum(1 for c in confidences if c < 0.5)

    print(f"   Ø¨Ø§Ù„Ø§ (â‰¥0.8):   {high_conf:3} ({high_conf / total * 100:5.1f}%)")
    print(f"   Ù…ØªÙˆØ³Ø· (0.5-0.8): {med_conf:3} ({med_conf / total * 100:5.1f}%)")
    print(f"   Ù¾Ø§ÛŒÛŒÙ† (<0.5):  {low_conf:3} ({low_conf / total * 100:5.1f}%)")

    # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø±
    print(f"\n5ï¸âƒ£ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø¯Ø± Ú©Ù„ Ù…ØªÙ†:")

    all_keywords = []
    for chunk in chunks:
        all_keywords.extend(chunk.get('keywords', []))

    keyword_freq = Counter(all_keywords)
    top_keywords = keyword_freq.most_common(10)

    for keyword, count in top_keywords:
        print(f"   {keyword:20}: {count} Ø¨Ø§Ø±")

    print("\n" + "=" * 60 + "\n")


def export_chunks_to_json(chunks: List[Dict], output_path: str):
    """Ø®Ø±ÙˆØ¬ÛŒ JSON Ø¨Ø±Ø§ÛŒ embedding Ø¨Ø¹Ø¯ÛŒ"""

    import json

    output_data = {
        'metadata': {
            'total_chunks': len(chunks),
            'total_words': sum(c['word_count'] for c in chunks),
            'avg_words_per_chunk': sum(c['word_count'] for c in chunks) / len(chunks) if chunks else 0
        },
        'chunks': chunks
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_path}")